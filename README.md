# rl-paper-study

Reinforcement Learning paper review study

## 1st paper list

Date | Paper | Presenter | Links
:---: | :---: | :---: | :---:
5/11 | Playing Atari with Deep Reinforcement Learning, Mnih et al, 2013. | Ingyun Ahn | [[paper]](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) [review] [code]
5/11 | Dueling Network Architectures for Deep Reinforcement Learning, Wang et al, 2015. | Jaeyoung Ahn | [[paper]](https://arxiv.org/abs/1511.06581) [review] [code]
5/18 | Deep Reinforcement Learning with Double Q-learning, Hasselt et al 2015. | Dohun Kim | [[paper]](https://arxiv.org/abs/1509.06461) [review] [code]
5/18 | Asynchronous Methods for Deep Reinforcement Learning, Mnih et al, 2016. | Seungyoun Shin | [[paper]](https://arxiv.org/abs/1602.01783) [review] [code]
5/25 | Proximal Policy Optimization Algorithms, Schulman et al, 2017. | Chris Ohk | [[paper]](https://arxiv.org/abs/1707.06347) [review] [code]
5/25 | Mastering the game of Go with deep neural networks and tree search, D. Silver et al, Nature, 2016. | Minseok Seong | [[paper]](https://www.nature.com/articles/nature16961) [review] [code]
6/1 | Curiosity-driven Exploration by Self-supervised Prediction, Pathak et al, 2017. | Haneul Choi | [[paper]](https://arxiv.org/abs/1705.05363) [review] [code]
6/1 | Mastering the game of Go without human knowledge, D. Silver et al, Nature, 2017. | Donggu Kang | [[paper]](https://www.nature.com/articles/nature24270) [review] [code]
6/8 | Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model, J. Schrittwieser et al, 2019. | Yunhyeok Kwak | [[paper]](https://arxiv.org/pdf/1911.08265) [review] [code]
6/8 | Model-based RL in Contextual Decision Processes: PAC bounds and Exponential Improvements over Model-free Approaches, Wen Sun et al, 2019. | Huiseong Ryu | [[paper]](https://arxiv.org/abs/1811.08540) [review] [code]
6/15 | Evolution Strategies as a Scalable Alternative to Reinforcement Learning, Salimans et al, 2017. | Chanhyuk Park | [[paper]](https://arxiv.org/abs/1703.03864) [review] [code]
6/15 | QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation, Kalashnikov et al, 2018. | Hyecheol Jang | [[paper]](https://arxiv.org/abs/1806.10293) [review] [code]

## Contact

You can contact me via e-mail (utilForever at gmail.com). I am always happy to answer questions or help with any issues you might have, and please be sure to share any additional work or your creations with me, I love seeing what other people are making.

## License

<img align="right" src="http://opensource.org/trademarks/opensource/OSI-Approved-License-100x137.png">

The class is licensed under the [MIT License](http://opensource.org/licenses/MIT):

Copyright (c) 2020 RL Paper Study Team

  * [Chris Ohk](http://www.github.com/utilForever)
  * [Yunhyeok Kwak](https://github.com/yun-kwak)
  * [Ingyun Ahn]
  * [Jaeyoung Ahn]
  * [Dohun Kim]
  * [Seungyoun Shin]
  * [Haneul Choi]
  * [Minseok Seong]
  * [Donggu Kang]
  * [Jiseong Lee]
  * [Chanhyuk Park]
  * [Hyecheol Jang]

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
